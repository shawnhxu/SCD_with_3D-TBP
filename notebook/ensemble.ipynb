{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble CNNs + CatBoost + LGBM models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What to find in this notebook:\n",
    "- Reload all trained models here.\n",
    "- Load in tabular as well as image data in test dataset.\n",
    "- Setup ensemble model.\n",
    "- Evaluate models with appropriate data.\n",
    "- Create submission.csv for Kaggle submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xusha\\anaconda3\\envs\\dsan6600_2\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import io\n",
    "import random\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 111\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "ROOT_DIR = \"../data\"\n",
    "TRAIN_CSV = f\"{ROOT_DIR}/train-metadata.csv\"\n",
    "TRAIN_HDF = f\"{ROOT_DIR}/train-image.hdf5\"\n",
    "TEST_CSV = f'{ROOT_DIR}/test-metadata.csv'\n",
    "TEST_HDF = f'{ROOT_DIR}/test-image.hdf5'\n",
    "SAMPLE = f'{ROOT_DIR}/sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"seed\": 42,\n",
    "    \"epochs\": 20,\n",
    "    \"img_size\": 224,\n",
    "    \"train_batch_size\": 150,\n",
    "    \"valid_batch_size\": 200,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"scheduler\": 'CosineAnnealingLR',\n",
    "    \"min_lr\": 1e-6,\n",
    "    \"T_max\": 500,\n",
    "    \"weight_decay\": 1e-6,\n",
    "    \"fold\" : 0,\n",
    "    \"n_fold\": 5,\n",
    "    \"n_accumulate\": 1,\n",
    "    \"device\": device,\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competitive Score metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, min_tpr: float=0.80):\n",
    "    v_gt = abs(np.asarray(solution.values)-1)\n",
    "    v_pred = np.array([1.0 - x for x in submission.values])\n",
    "    max_fpr = abs(1-min_tpr)\n",
    "    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "    # change scale from [0.5, 1.0] to [0.5 * max_fpr**2, max_fpr]\n",
    "    # https://math.stackexchange.com/questions/914823/shift-numbers-into-a-different-range\n",
    "    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "    return partial_auc\n",
    "\n",
    "def custom_lgbm_metric(y_true, y_hat):\n",
    "    # TODO: Refactor with the above.\n",
    "    min_tpr = 0.80\n",
    "    v_gt = abs(y_true-1)\n",
    "    v_pred = np.array([1.0 - x for x in y_hat])\n",
    "    max_fpr = abs(1-min_tpr)\n",
    "    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n",
    "    # change scale from [0.5, 1.0] to [0.5 * max_fpr**2, max_fpr]\n",
    "    # https://math.stackexchange.com/questions/914823/shift-numbers-into-a-different-range\n",
    "    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n",
    "    return \"pauc80\", partial_auc, True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init CNN Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RESNET152\n",
    "\"\"\"\n",
    "\n",
    "class CustomResNet152(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomResNet152, self).__init__()\n",
    "        # Load a pre-trained ResNet-152 model\n",
    "        self.base_model = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # remove last fully connected layer for our purposes\n",
    "        self.features = nn.Sequential(*list(self.base_model.children())[:-2])\n",
    "\n",
    "        # Classifier that includes flattening the feature map and linear layer for class prediction\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2048, 1),  # output single value for prob calculation\n",
    "            nn.Sigmoid()  # sigmoid activation for probability\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # extract features\n",
    "        x = self.features(x)\n",
    "        # classify features\n",
    "        output = self.classifier(x)\n",
    "        return output\n",
    "\n",
    "model1 = CustomResNet152()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RESNET50\n",
    "\"\"\"\n",
    "\n",
    "class CustomResNet50(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomResNet50, self).__init__()\n",
    "        # Load a pre-trained ResNet-152 model\n",
    "        self.base_model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # remove last fully connected layer for our purposes\n",
    "        self.features = nn.Sequential(*list(self.base_model.children())[:-2])\n",
    "\n",
    "        # Classifier that includes flattening the feature map and linear layer for class prediction\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2048, 1),  # output single value for prob calculation\n",
    "            nn.Sigmoid()  # sigmoid activation for probability\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # extract features\n",
    "        x = self.features(x)\n",
    "        # classify features\n",
    "        output = self.classifier(x)\n",
    "        return output\n",
    "\n",
    "model2 = CustomResNet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MobileNetV2\n",
    "\"\"\"\n",
    "\n",
    "class CustomMobileNetV2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomMobileNetV2, self).__init__()\n",
    "        self.base_model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        self.features = self.base_model.features\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1280, 1),\n",
    "            nn.Sigmoid()  \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        output = self.classifier(x)\n",
    "        return output\n",
    "\n",
    "model3 = CustomMobileNetV2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MNASNet-1.0\n",
    "\"\"\"\n",
    "\n",
    "class CustomMNASNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomMNASNet, self).__init__()\n",
    "        self.base_model = models.mnasnet1_0(weights=models.MNASNet1_0_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        self.features = self.base_model.layers\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1280, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        output = self.classifier(x)\n",
    "        return output\n",
    "    \n",
    "model4 = CustomMNASNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EfficientNetB4\n",
    "\"\"\"\n",
    "\n",
    "class CustomEfficientNetB4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomEfficientNetB4, self).__init__()\n",
    "        self.base_model = models.efficientnet_b4(weights=models.EfficientNet_B4_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        self.base_model.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.4), \n",
    "            nn.Linear(self.base_model.classifier[1].in_features, 1),\n",
    "            nn.Sigmoid() \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.base_model(x)\n",
    "        return output\n",
    "    \n",
    "model5 = CustomEfficientNetB4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DenseNet121\n",
    "\"\"\"\n",
    "\n",
    "class CustomDenseNet121(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomDenseNet121, self).__init__()\n",
    "        self.base_model = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        self.base_model.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 1), \n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.base_model(x)\n",
    "        return output\n",
    "\n",
    "model6 = CustomDenseNet121()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CNNs in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.load_state_dict(torch.load('./output/res152_ISIC_best.pth', map_location=torch.device('cuda')))\n",
    "model2.load_state_dict(torch.load('./output/res50_ISIC_best.pth', map_location=torch.device('cuda')))\n",
    "model3.load_state_dict(torch.load('./output/mobileV2_ISIC_best.pth', map_location=torch.device('cuda')))\n",
    "model4.load_state_dict(torch.load('./output/mnas1_0_ISIC_best.pth', map_location=torch.device('cuda')))\n",
    "model5.load_state_dict(torch.load('./output/effB4_ISIC_best.pth', map_location=torch.device('cuda')))\n",
    "model6.load_state_dict(torch.load('./output/Dense121_ISIC_best.pth', map_location=torch.device('cuda')))\n",
    "\n",
    "model1 = model1.to(device)\n",
    "model2 = model2.to(device)\n",
    "model3 = model3.to(device)\n",
    "model4 = model4.to(device)\n",
    "model5 = model5.to(device)\n",
    "model6 = model6.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CatBoost and LGBM in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x1352aa99b70>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Catboost\n",
    "\"\"\"\n",
    "\n",
    "model7 = CatBoostClassifier()\n",
    "model7.load_model(\"./output/catboost_model.cbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LGBM\n",
    "\"\"\"\n",
    "# Load the Booster model\n",
    "model8 = lgb.Booster(model_file=\"./output/lightgbm_model.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinput data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Files: 100%|██████████| 401059/401059 [02:19<00:00, 2880.98it/s]\n",
      "Reading Files: 100%|██████████| 3/3 [00:00<00:00, 3007.39it/s]\n",
      "C:\\Users\\xusha\\AppData\\Local\\Temp\\ipykernel_1369720\\2087674646.py:22: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_metadata = pd.read_csv(TRAIN_CSV)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import just the testing dataset\n",
    "\"\"\"\n",
    "\n",
    "def read_images_from_hdf5(file_path):\n",
    "    images = {}\n",
    "    try:\n",
    "        with h5py.File(file_path, 'r') as file:\n",
    "            for key in tqdm(file.keys(), desc=\"Reading Files\"):\n",
    "                try:\n",
    "                    image_data = file[key][()]\n",
    "                    image = Image.open(io.BytesIO(image_data))\n",
    "                    images[key] = image\n",
    "                except Exception as e:\n",
    "                    print(f\"Error! from {key}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occured while reading files : {e}\")\n",
    "    \n",
    "    return images\n",
    "train_images = read_images_from_hdf5(TRAIN_HDF)\n",
    "test_images = read_images_from_hdf5(TEST_HDF)\n",
    "train_metadata = pd.read_csv(TRAIN_CSV)\n",
    "test_metadata = pd.read_csv(TEST_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define oversampling and undersampling strategy\n",
    "oversample = RandomOverSampler(sampling_strategy=0.003)  \n",
    "undersample = RandomUnderSampler(sampling_strategy=0.9)  \n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('oversample', oversample),\n",
    "    ('undersample', undersample)\n",
    "])\n",
    "\n",
    "# Fit and transform the data\n",
    "X_sample, y_sample = pipeline.fit_resample(train_metadata.drop([\"target\"],axis=1),train_metadata[\"target\"])\n",
    "X_sample[\"target\"] = y_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import test image dataloaders for CNN evaluation\n",
    "\"\"\"\n",
    "\n",
    "def remove_hair(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (9,9))\n",
    "    blackhat = cv2.morphologyEx(gray,cv2.MORPH_BLACKHAT,kernel)\n",
    "\n",
    "    _, thresh = cv2.threshold(blackhat, 10 ,255, cv2.THRESH_BINARY)\n",
    "    inpainted_image = cv2.inpaint(image, thresh, 1, cv2.INPAINT_TELEA)\n",
    "    return inpainted_image\n",
    "\n",
    "\n",
    "class ISIC_2024(Dataset):\n",
    "    def __init__(self,pil_images,metadata,transform=None,test=False):\n",
    "        self.pil_images = pil_images\n",
    "        self.metadata = metadata\n",
    "        self.transform = transform\n",
    "        self.test= test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    # This function from https://www.kaggle.com/competitions/isic-2024-challenge/discussion/519735\n",
    "    def remove_hair(image):\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (9,9))\n",
    "        blackhat = cv2.morphologyEx(gray,cv2.MORPH_BLACKHAT,kernel)\n",
    "\n",
    "        _, thresh = cv2.threshold(blackhat, 10 ,255, cv2.THRESH_BINARY)\n",
    "        inpainted_image = cv2.inpaint(image, thresh, 1, cv2.INPAINT_TELEA)\n",
    "        return inpainted_image\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        isic_id = self.metadata.iloc[idx,0]\n",
    "        cleaned_image = remove_hair(np.array(self.pil_images[isic_id]))\n",
    "        image = Image.fromarray(cleaned_image)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.test:\n",
    "            return image, isic_id\n",
    "        label = self.metadata.iloc[idx,-1]\n",
    "        return image,label,isic_id\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['img_size'], CONFIG['img_size'])),\n",
    "    transforms.RandomHorizontalFlip(p=0.5), \n",
    "    transforms.RandomVerticalFlip(p=0.5),    \n",
    "    transforms.RandomRotation(20),           \n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.5, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),                   \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # commonly used mean and std calculated from ImageNet\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "     transforms.Resize((CONFIG['img_size'], CONFIG['img_size'])),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # commonly used mean and std calculated from ImageNet\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ISIC_2024(train_images, X_sample, transform = train_transforms)\n",
    "test_dataset = ISIC_2024(test_images, test_metadata, transform = test_transforms, test = True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xusha\\AppData\\Local\\Temp\\ipykernel_1369720\\1555227886.py:1: DtypeWarning: Columns (51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_train = pd.read_csv(\"../data/train-metadata.csv\")\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../data/train-metadata.csv\")\n",
    "df_test = pd.read_csv(\"../data/test-metadata.csv\")\n",
    "\n",
    "def feature_engineering(df):\n",
    "    df[\"lesion_size_ratio\"]              = df[\"tbp_lv_minorAxisMM\"] / df[\"clin_size_long_diam_mm\"]\n",
    "    df[\"lesion_shape_index\"]             = df[\"tbp_lv_areaMM2\"] / (df[\"tbp_lv_perimeterMM\"] ** 2)\n",
    "    df[\"hue_contrast\"]                   = (df[\"tbp_lv_H\"] - df[\"tbp_lv_Hext\"]).abs()\n",
    "    df[\"luminance_contrast\"]             = (df[\"tbp_lv_L\"] - df[\"tbp_lv_Lext\"]).abs()\n",
    "    df[\"lesion_color_difference\"]        = np.sqrt(df[\"tbp_lv_deltaA\"] ** 2 + df[\"tbp_lv_deltaB\"] ** 2 + df[\"tbp_lv_deltaL\"] ** 2)\n",
    "    df[\"border_complexity\"]              = df[\"tbp_lv_norm_border\"] + df[\"tbp_lv_symm_2axis\"]\n",
    "    df[\"color_uniformity\"]               = df[\"tbp_lv_color_std_mean\"] / df[\"tbp_lv_radial_color_std_max\"]\n",
    "    \n",
    "    df[\"3d_position_distance\"]           = np.sqrt(df[\"tbp_lv_x\"] ** 2 + df[\"tbp_lv_y\"] ** 2 + df[\"tbp_lv_z\"] ** 2) \n",
    "    df[\"perimeter_to_area_ratio\"]        = df[\"tbp_lv_perimeterMM\"] / df[\"tbp_lv_areaMM2\"]\n",
    "    df[\"area_to_perimeter_ratio\"]        = df[\"tbp_lv_areaMM2\"] / df[\"tbp_lv_perimeterMM\"]\n",
    "    df[\"lesion_visibility_score\"]        = df[\"tbp_lv_deltaLBnorm\"] + df[\"tbp_lv_norm_color\"]\n",
    "    df[\"combined_anatomical_site\"]       = df[\"anatom_site_general\"] + \"_\" + df[\"tbp_lv_location\"]\n",
    "    df[\"symmetry_border_consistency\"]    = df[\"tbp_lv_symm_2axis\"] * df[\"tbp_lv_norm_border\"]\n",
    "    df[\"consistency_symmetry_border\"]    = df[\"tbp_lv_symm_2axis\"] * df[\"tbp_lv_norm_border\"] / (df[\"tbp_lv_symm_2axis\"] + df[\"tbp_lv_norm_border\"])\n",
    "    \n",
    "    df[\"color_consistency\"]              = df[\"tbp_lv_stdL\"] / df[\"tbp_lv_Lext\"]\n",
    "    df[\"consistency_color\"]              = df[\"tbp_lv_stdL\"] * df[\"tbp_lv_Lext\"] / (df[\"tbp_lv_stdL\"] + df[\"tbp_lv_Lext\"])\n",
    "    df[\"size_age_interaction\"]           = df[\"clin_size_long_diam_mm\"] * df[\"age_approx\"]\n",
    "    df[\"hue_color_std_interaction\"]      = df[\"tbp_lv_H\"] * df[\"tbp_lv_color_std_mean\"]\n",
    "    df[\"lesion_severity_index\"]          = (df[\"tbp_lv_norm_border\"] + df[\"tbp_lv_norm_color\"] + df[\"tbp_lv_eccentricity\"]) / 3\n",
    "    df[\"shape_complexity_index\"]         = df[\"border_complexity\"] + df[\"lesion_shape_index\"]\n",
    "    df[\"color_contrast_index\"]           = df[\"tbp_lv_deltaA\"] + df[\"tbp_lv_deltaB\"] + df[\"tbp_lv_deltaL\"] + df[\"tbp_lv_deltaLBnorm\"]\n",
    "    \n",
    "    df[\"log_lesion_area\"]                = np.log(df[\"tbp_lv_areaMM2\"] + 1)\n",
    "    df[\"normalized_lesion_size\"]         = df[\"clin_size_long_diam_mm\"] / df[\"age_approx\"]\n",
    "    df[\"mean_hue_difference\"]            = (df[\"tbp_lv_H\"] + df[\"tbp_lv_Hext\"]) / 2\n",
    "    df[\"std_dev_contrast\"]               = np.sqrt((df[\"tbp_lv_deltaA\"] ** 2 + df[\"tbp_lv_deltaB\"] ** 2 + df[\"tbp_lv_deltaL\"] ** 2) / 3)\n",
    "    df[\"color_shape_composite_index\"]    = (df[\"tbp_lv_color_std_mean\"] + df[\"tbp_lv_area_perim_ratio\"] + df[\"tbp_lv_symm_2axis\"]) / 3\n",
    "    df[\"3d_lesion_orientation\"]          = np.arctan2(df_train[\"tbp_lv_y\"], df_train[\"tbp_lv_x\"])\n",
    "    df[\"overall_color_difference\"]       = (df[\"tbp_lv_deltaA\"] + df[\"tbp_lv_deltaB\"] + df[\"tbp_lv_deltaL\"]) / 3\n",
    "    \n",
    "    df[\"symmetry_perimeter_interaction\"] = df[\"tbp_lv_symm_2axis\"] * df[\"tbp_lv_perimeterMM\"]\n",
    "    df[\"comprehensive_lesion_index\"]     = (df[\"tbp_lv_area_perim_ratio\"] + df[\"tbp_lv_eccentricity\"] + df[\"tbp_lv_norm_color\"] + df[\"tbp_lv_symm_2axis\"]) / 4\n",
    "    df[\"color_variance_ratio\"]           = df[\"tbp_lv_color_std_mean\"] / df[\"tbp_lv_stdLExt\"]\n",
    "    df[\"border_color_interaction\"]       = df[\"tbp_lv_norm_border\"] * df[\"tbp_lv_norm_color\"]\n",
    "    df[\"size_color_contrast_ratio\"]      = df[\"clin_size_long_diam_mm\"] / df[\"tbp_lv_deltaLBnorm\"]\n",
    "    df[\"age_normalized_nevi_confidence\"] = df[\"tbp_lv_nevi_confidence\"] / df[\"age_approx\"]\n",
    "    df[\"color_asymmetry_index\"]          = df[\"tbp_lv_radial_color_std_max\"] * df[\"tbp_lv_symm_2axis\"]\n",
    "    \n",
    "    df[\"3d_volume_approximation\"]        = df[\"tbp_lv_areaMM2\"] * np.sqrt(df[\"tbp_lv_x\"]**2 + df[\"tbp_lv_y\"]**2 + df[\"tbp_lv_z\"]**2)\n",
    "    df[\"color_range\"]                    = (df[\"tbp_lv_L\"] - df[\"tbp_lv_Lext\"]).abs() + (df[\"tbp_lv_A\"] - df[\"tbp_lv_Aext\"]).abs() + (df[\"tbp_lv_B\"] - df[\"tbp_lv_Bext\"]).abs()\n",
    "    df[\"shape_color_consistency\"]        = df[\"tbp_lv_eccentricity\"] * df[\"tbp_lv_color_std_mean\"]\n",
    "    df[\"border_length_ratio\"]            = df[\"tbp_lv_perimeterMM\"] / (2 * np.pi * np.sqrt(df[\"tbp_lv_areaMM2\"] / np.pi))\n",
    "    df[\"age_size_symmetry_index\"]        = df[\"age_approx\"] * df[\"clin_size_long_diam_mm\"] * df[\"tbp_lv_symm_2axis\"]\n",
    "    df[\"index_age_size_symmetry\"]        = df[\"age_approx\"] * df[\"tbp_lv_areaMM2\"] * df[\"tbp_lv_symm_2axis\"]\n",
    "\n",
    "    \n",
    "    new_num_cols = [\n",
    "        \"lesion_size_ratio\",             # tbp_lv_minorAxisMM      / clin_size_long_diam_mm\n",
    "        \"lesion_shape_index\",            # tbp_lv_areaMM2          / tbp_lv_perimeterMM **2\n",
    "        \"hue_contrast\",                  # tbp_lv_H                - tbp_lv_Hext              abs\n",
    "        \"luminance_contrast\",            # tbp_lv_L                - tbp_lv_Lext              abs\n",
    "        \"lesion_color_difference\",       # tbp_lv_deltaA **2       + tbp_lv_deltaB **2 + tbp_lv_deltaL **2  sqrt  \n",
    "        \"border_complexity\",             # tbp_lv_norm_border      + tbp_lv_symm_2axis\n",
    "        \"color_uniformity\",              # tbp_lv_color_std_mean   / tbp_lv_radial_color_std_max\n",
    "        \n",
    "        \"3d_position_distance\",          # tbp_lv_x **2 + tbp_lv_y **2 + tbp_lv_z **2  sqrt\n",
    "        \"perimeter_to_area_ratio\",       # tbp_lv_perimeterMM      / tbp_lv_areaMM2\n",
    "        \"area_to_perimeter_ratio\",       # tbp_lv_areaMM2          / tbp_lv_perimeterMM\n",
    "        \"lesion_visibility_score\",       # tbp_lv_deltaLBnorm      + tbp_lv_norm_color\n",
    "        # \"combined_anatomical_site\"      # anatom_site_general     + \"_\" + tbp_lv_location ! categorical feature\n",
    "        \"symmetry_border_consistency\",   # tbp_lv_symm_2axis       * tbp_lv_norm_border\n",
    "        \"consistency_symmetry_border\",   # tbp_lv_symm_2axis       * tbp_lv_norm_border / (tbp_lv_symm_2axis + tbp_lv_norm_border)\n",
    "        \n",
    "        \"color_consistency\",             # tbp_lv_stdL             / tbp_lv_Lext\n",
    "        \"consistency_color\",             # tbp_lv_stdL*tbp_lv_Lext / tbp_lv_stdL + tbp_lv_Lext\n",
    "        \"size_age_interaction\",          # clin_size_long_diam_mm  * age_approx\n",
    "        \"hue_color_std_interaction\",     # tbp_lv_H                * tbp_lv_color_std_mean\n",
    "        \"lesion_severity_index\",         # tbp_lv_norm_border      + tbp_lv_norm_color + tbp_lv_eccentricity / 3\n",
    "        \"shape_complexity_index\",        # border_complexity       + lesion_shape_index\n",
    "        \"color_contrast_index\",          # tbp_lv_deltaA + tbp_lv_deltaB + tbp_lv_deltaL + tbp_lv_deltaLBnorm\n",
    "        \n",
    "        \"log_lesion_area\",               # tbp_lv_areaMM2          + 1  np.log\n",
    "        \"normalized_lesion_size\",        # clin_size_long_diam_mm  / age_approx\n",
    "        \"mean_hue_difference\",           # tbp_lv_H                + tbp_lv_Hext    / 2\n",
    "        \"std_dev_contrast\",              # tbp_lv_deltaA **2 + tbp_lv_deltaB **2 + tbp_lv_deltaL **2   / 3  np.sqrt\n",
    "        \"color_shape_composite_index\",   # tbp_lv_color_std_mean   + bp_lv_area_perim_ratio + tbp_lv_symm_2axis   / 3\n",
    "        \"3d_lesion_orientation\",         # tbp_lv_y                , tbp_lv_x  np.arctan2\n",
    "        \"overall_color_difference\",      # tbp_lv_deltaA           + tbp_lv_deltaB + tbp_lv_deltaL   / 3\n",
    "        \n",
    "        \"symmetry_perimeter_interaction\",# tbp_lv_symm_2axis       * tbp_lv_perimeterMM\n",
    "        \"comprehensive_lesion_index\",    # tbp_lv_area_perim_ratio + tbp_lv_eccentricity + bp_lv_norm_color + tbp_lv_symm_2axis   / 4\n",
    "        \"color_variance_ratio\",          # tbp_lv_color_std_mean   / tbp_lv_stdLExt\n",
    "        \"border_color_interaction\",      # tbp_lv_norm_border      * tbp_lv_norm_color\n",
    "        \"size_color_contrast_ratio\",     # clin_size_long_diam_mm  / tbp_lv_deltaLBnorm\n",
    "        \"age_normalized_nevi_confidence\",# tbp_lv_nevi_confidence  / age_approx\n",
    "        \"color_asymmetry_index\",         # tbp_lv_symm_2axis       * tbp_lv_radial_color_std_max\n",
    "        \n",
    "        \"3d_volume_approximation\",       # tbp_lv_areaMM2          * sqrt(tbp_lv_x**2 + tbp_lv_y**2 + tbp_lv_z**2)\n",
    "        \"color_range\",                   # abs(tbp_lv_L - tbp_lv_Lext) + abs(tbp_lv_A - tbp_lv_Aext) + abs(tbp_lv_B - tbp_lv_Bext)\n",
    "        \"shape_color_consistency\",       # tbp_lv_eccentricity     * tbp_lv_color_std_mean\n",
    "        \"border_length_ratio\",           # tbp_lv_perimeterMM      / pi * sqrt(tbp_lv_areaMM2 / pi)\n",
    "        \"age_size_symmetry_index\",       # age_approx              * clin_size_long_diam_mm * tbp_lv_symm_2axis\n",
    "         #\"index_age_size_symmetry\",      # age_approx              * sqrt(tbp_lv_areaMM2 * tbp_lv_symm_2axis)\n",
    "        \"index_age_size_symmetry\",       # age_approx              * tbp_lv_areaMM2 * tbp_lv_symm_2axis\n",
    "\n",
    "    ]\n",
    "    \n",
    "    new_cat_cols = [\"combined_anatomical_site\"]\n",
    "    \n",
    "    return df, new_num_cols, new_cat_cols\n",
    "\n",
    "num_cols = [\n",
    "    'age_approx',                        # Approximate age of patient at time of imaging.\n",
    "    'clin_size_long_diam_mm',            # Maximum diameter of the lesion (mm).+\n",
    "    'tbp_lv_A',                          # A inside  lesion.+\n",
    "    'tbp_lv_Aext',                       # A outside lesion.+\n",
    "    'tbp_lv_B',                          # B inside  lesion.+\n",
    "    'tbp_lv_Bext',                       # B outside lesion.+ \n",
    "    'tbp_lv_C',                          # Chroma inside  lesion.+\n",
    "    'tbp_lv_Cext',                       # Chroma outside lesion.+\n",
    "    'tbp_lv_H',                          # Hue inside the lesion; calculated as the angle of A* and B* in LAB* color space. Typical values range from 25 (red) to 75 (brown).+\n",
    "    'tbp_lv_Hext',                       # Hue outside lesion.+\n",
    "    'tbp_lv_L',                          # L inside lesion.+\n",
    "    'tbp_lv_Lext',                       # L outside lesion.+\n",
    "    'tbp_lv_areaMM2',                    # Area of lesion (mm^2).+\n",
    "    'tbp_lv_area_perim_ratio',           # Border jaggedness, the ratio between lesions perimeter and area. Circular lesions will have low values; irregular shaped lesions will have higher values. Values range 0-10.+\n",
    "    'tbp_lv_color_std_mean',             # Color irregularity, calculated as the variance of colors within the lesion's boundary.\n",
    "    'tbp_lv_deltaA',                     # Average A contrast (inside vs. outside lesion).+\n",
    "    'tbp_lv_deltaB',                     # Average B contrast (inside vs. outside lesion).+\n",
    "    'tbp_lv_deltaL',                     # Average L contrast (inside vs. outside lesion).+\n",
    "    'tbp_lv_deltaLB',                    #\n",
    "    'tbp_lv_deltaLBnorm',                # Contrast between the lesion and its immediate surrounding skin. Low contrast lesions tend to be faintly visible such as freckles; high contrast lesions tend to be those with darker pigment. Calculated as the average delta LB of the lesion relative to its immediate background in LAB* color space. Typical values range from 5.5 to 25.+\n",
    "    'tbp_lv_eccentricity',               # Eccentricity.+\n",
    "    'tbp_lv_minorAxisMM',                # Smallest lesion diameter (mm).+\n",
    "    'tbp_lv_nevi_confidence',            # Nevus confidence score (0-100 scale) is a convolutional neural network classifier estimated probability that the lesion is a nevus. The neural network was trained on approximately 57,000 lesions that were classified and labeled by a dermatologist.+,++\n",
    "    'tbp_lv_norm_border',                # Border irregularity (0-10 scale); the normalized average of border jaggedness and asymmetry.+\n",
    "    'tbp_lv_norm_color',                 # Color variation (0-10 scale); the normalized average of color asymmetry and color irregularity.+\n",
    "    'tbp_lv_perimeterMM',                # Perimeter of lesion (mm).+\n",
    "    'tbp_lv_radial_color_std_max',       # Color asymmetry, a measure of asymmetry of the spatial distribution of color within the lesion. This score is calculated by looking at the average standard deviation in LAB* color space within concentric rings originating from the lesion center. Values range 0-10.+\n",
    "    'tbp_lv_stdL',                       # Standard deviation of L inside  lesion.+\n",
    "    'tbp_lv_stdLExt',                    # Standard deviation of L outside lesion.+\n",
    "    'tbp_lv_symm_2axis',                 # Border asymmetry; a measure of asymmetry of the lesion's contour about an axis perpendicular to the lesion's most symmetric axis. Lesions with two axes of symmetry will therefore have low scores (more symmetric), while lesions with only one or zero axes of symmetry will have higher scores (less symmetric). This score is calculated by comparing opposite halves of the lesion contour over many degrees of rotation. The angle where the halves are most similar identifies the principal axis of symmetry, while the second axis of symmetry is perpendicular to the principal axis. Border asymmetry is reported as the asymmetry value about this second axis. Values range 0-10.+\n",
    "    'tbp_lv_symm_2axis_angle',           # Lesion border asymmetry angle.+\n",
    "    'tbp_lv_x',                          # X-coordinate of the lesion on 3D TBP.+\n",
    "    'tbp_lv_y',                          # Y-coordinate of the lesion on 3D TBP.+\n",
    "    'tbp_lv_z',                          # Z-coordinate of the lesion on 3D TBP.+\n",
    "]\n",
    "\n",
    "df_train[num_cols] = df_train[num_cols].fillna(df_train[num_cols].median())\n",
    "df_test [num_cols] = df_test [num_cols].fillna(df_train[num_cols].median())\n",
    "\n",
    "df_train, new_num_cols, new_cat_cols = feature_engineering(df_train.copy())\n",
    "df_test, _, _                        = feature_engineering(df_test.copy())\n",
    "\n",
    "num_cols += new_num_cols\n",
    "\n",
    "# anatom_site_general\n",
    "cat_cols = [\"sex\", \"tbp_tile_type\", \"tbp_lv_location\", \"tbp_lv_location_simple\"] + new_cat_cols\n",
    "#train_cols = num_cols + cat_cols\n",
    "train_cols = num_cols + cat_cols\n",
    "\n",
    "category_encoder = OrdinalEncoder(\n",
    "    categories='auto',\n",
    "    dtype=int,\n",
    "    handle_unknown='use_encoded_value',\n",
    "    unknown_value=-2,\n",
    "    encoded_missing_value=-1,\n",
    ")\n",
    "\n",
    "X_cat1 = category_encoder.fit_transform(df_test[cat_cols])\n",
    "X_cat2 = category_encoder.fit_transform(df_train[cat_cols])\n",
    "for c, cat_col in enumerate(cat_cols):\n",
    "    df_test[cat_col] = X_cat1[:, c]\n",
    "    df_train[cat_col] = X_cat2[:, c]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble model (Averaging)\n",
    "\n",
    "There are 8 models to ensemble through averaging predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_models = [model1, model2, model3, model4, model5, model6]\n",
    "\n",
    "def get_test_predictions(dataloader, model, device, model_type='cnn'):\n",
    "    predictions = []\n",
    "    if model_type == 'cnn':\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data, _ in dataloader:\n",
    "                data = data.to(device)\n",
    "                pred = model(data).cpu().numpy()\n",
    "                predictions.append(pred)\n",
    "    elif model_type == 'cat':\n",
    "        pred = model.predict_proba(dataloader)[:, 1]\n",
    "        predictions.append(pred)\n",
    "    elif model_type == 'lgbm':\n",
    "        pred = model.predict(dataloader, raw_score=False)\n",
    "        predictions.append(pred)\n",
    "    return np.concatenate(predictions, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "combined cnn preds\n"
     ]
    }
   ],
   "source": [
    "print('start')\n",
    "# Get CNN predictions\n",
    "cnn_preds_list = [get_test_predictions(test_loader, model, device, model_type='cnn') for model in cnn_models]\n",
    "cnn_preds_list = np.stack(cnn_preds_list, axis=1).reshape(-1, len(cnn_models)) #reshape for concat later\n",
    "print('combined cnn preds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catboost done\n",
      "lgbm done\n"
     ]
    }
   ],
   "source": [
    "# Get tabular model predictions\n",
    "catboost_preds = get_test_predictions(df_test[train_cols], model7, device, model_type='cat').reshape(-1, 1)\n",
    "print('catboost done')\n",
    "lgbm_preds = get_test_predictions(df_test[train_cols], model8, device, model_type='lgbm').reshape(-1, 1)\n",
    "print('lgbm done')\n",
    "# Combine all predictions into a feature matrix\n",
    "ensemble_preds = np.column_stack([cnn_preds_list, catboost_preds, lgbm_preds])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep submission\n",
    "df_subm = pd.read_csv('../data/sample_submission.csv', index_col='isic_id')\n",
    "\n",
    "# weighted average\n",
    "weights = [0.1, 0.1, 0.05, 0.05, 0.05, 0.05, 0.3, 0.3]  # Example weights\n",
    "df_subm['target'] = np.average(ensemble_preds, axis=1, weights=weights)\n",
    "\n",
    "#output submission csv file\n",
    "df_subm.to_csv('./output/submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_cnn(model, dataloader, device):\n",
    "\n",
    "    #init eval mode\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_ids = []\n",
    "\n",
    "    for data, id in tqdm(dataloader):\n",
    "        \n",
    "        #init data and target into cuda\n",
    "        data = data.to(device)\n",
    "\n",
    "        #predict using input data\n",
    "        curr_pred = model(data)\n",
    "        print(curr_pred)\n",
    "        _, predicted = torch.max(curr_pred, 1) \n",
    "\n",
    "        #store preds and ids\n",
    "        all_preds.append(predicted.cpu())\n",
    "        all_ids.extend(id)\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0158],\n",
      "        [0.0173],\n",
      "        [0.0021]], device='cuda:0', grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_cnn(model1, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.15957845e-05 3.14439194e-05 9.77989859e-05]\n"
     ]
    }
   ],
   "source": [
    "test1 = model7.predict_proba(df_test[train_cols])[:, 1]\n",
    "print(test1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model8\u001b[38;5;241m.\u001b[39mpredict(df_test[\u001b[43mtrain_cols\u001b[49m], raw_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_cols' is not defined"
     ]
    }
   ],
   "source": [
    "model8.predict(df_test[train_cols], raw_score=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ISIC_0015670', 'ISIC_0015845', 'ISIC_0015864', 'ISIC_0015902', 'ISIC_0024200', 'ISIC_0035502', 'ISIC_0051648', 'ISIC_0051665', 'ISIC_0051710', 'ISIC_0051758', 'ISIC_0051812', 'ISIC_0051822', 'ISIC_0051896', 'ISIC_0051897', 'ISIC_0051958', 'ISIC_0051983', 'ISIC_0052003', 'ISIC_0052004', 'ISIC_0052026', 'ISIC_0052042', 'ISIC_0052068', 'ISIC_0052094', 'ISIC_0052109', 'ISIC_0052122', 'ISIC_0052164', 'ISIC_0052205', 'ISIC_0052213', 'ISIC_0052220', 'ISIC_0052231', 'ISIC_0052241', 'ISIC_0052259', 'ISIC_0052270']\n",
      "['ISIC_0052310', 'ISIC_0052313', 'ISIC_0052328', 'ISIC_0052332', 'ISIC_0052355', 'ISIC_0052357', 'ISIC_0052367', 'ISIC_0061318', 'ISIC_0062556', 'ISIC_0065755', 'ISIC_0067881', 'ISIC_0068212', 'ISIC_0070972', 'ISIC_0071851', 'ISIC_0071852', 'ISIC_0073261', 'ISIC_0073270', 'ISIC_0073301', 'ISIC_0073316', 'ISIC_0073364', 'ISIC_0073396', 'ISIC_0073412', 'ISIC_0073426', 'ISIC_0073467', 'ISIC_0073505', 'ISIC_0073511', 'ISIC_0073521', 'ISIC_0073522', 'ISIC_0073555', 'ISIC_0073642', 'ISIC_0073665', 'ISIC_0073672']\n",
      "['ISIC_0073674', 'ISIC_0073694', 'ISIC_0073697', 'ISIC_0073729', 'ISIC_0073774', 'ISIC_0073777', 'ISIC_0073865', 'ISIC_0073877', 'ISIC_0073934', 'ISIC_0073937', 'ISIC_0073944', 'ISIC_0073975', 'ISIC_0073989', 'ISIC_0074075', 'ISIC_0074083', 'ISIC_0074118', 'ISIC_0074139', 'ISIC_0074209', 'ISIC_0074226', 'ISIC_0074298', 'ISIC_0074366', 'ISIC_0074374', 'ISIC_0074413', 'ISIC_0074437', 'ISIC_0074443', 'ISIC_0074460', 'ISIC_0074505', 'ISIC_0074538', 'ISIC_0074553', 'ISIC_0074611', 'ISIC_0074621', 'ISIC_0074664']\n",
      "['ISIC_0074666', 'ISIC_0075664', 'ISIC_0075676', 'ISIC_0075713', 'ISIC_0075726', 'ISIC_0075770', 'ISIC_0075789', 'ISIC_0075799', 'ISIC_0075822', 'ISIC_0075868', 'ISIC_0075905', 'ISIC_0075969', 'ISIC_0075982', 'ISIC_0076018', 'ISIC_0076086', 'ISIC_0076143', 'ISIC_0076172', 'ISIC_0076197', 'ISIC_0076213', 'ISIC_0076220', 'ISIC_0076236', 'ISIC_0076244', 'ISIC_0076245', 'ISIC_0076259', 'ISIC_0076270', 'ISIC_0076308', 'ISIC_0076312', 'ISIC_0076319', 'ISIC_0076353', 'ISIC_0076365', 'ISIC_0076380', 'ISIC_0076389']\n",
      "['ISIC_0076410', 'ISIC_0076412', 'ISIC_0076429', 'ISIC_0076432', 'ISIC_0076435', 'ISIC_0076450', 'ISIC_0076494', 'ISIC_0076505', 'ISIC_0076516', 'ISIC_0076541', 'ISIC_0076547', 'ISIC_0076625', 'ISIC_0076635', 'ISIC_0076681', 'ISIC_0076758', 'ISIC_0076767', 'ISIC_0076781', 'ISIC_0076835', 'ISIC_0076877', 'ISIC_0076922', 'ISIC_0076936', 'ISIC_0076969', 'ISIC_0077052', 'ISIC_0077064', 'ISIC_0077130', 'ISIC_0077158', 'ISIC_0077178', 'ISIC_0077197', 'ISIC_0077243', 'ISIC_0077295', 'ISIC_0077306', 'ISIC_0077326']\n",
      "['ISIC_0077349', 'ISIC_0077373', 'ISIC_0077385', 'ISIC_0077392', 'ISIC_0077394', 'ISIC_0077418', 'ISIC_0077462', 'ISIC_0077489', 'ISIC_0077502', 'ISIC_0077575', 'ISIC_0077628', 'ISIC_0077654', 'ISIC_0077694', 'ISIC_0077696', 'ISIC_0077707', 'ISIC_0077748', 'ISIC_0077751', 'ISIC_0077768', 'ISIC_0077778', 'ISIC_0077812', 'ISIC_0077816', 'ISIC_0077857', 'ISIC_0077872', 'ISIC_0077915', 'ISIC_0077932', 'ISIC_0077935', 'ISIC_0077947', 'ISIC_0077965', 'ISIC_0077975', 'ISIC_0078026', 'ISIC_0078044', 'ISIC_0078045']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, label, ids \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(ids)\n",
      "File \u001b[1;32mc:\\Users\\xusha\\anaconda3\\envs\\dsan6600_2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\xusha\\anaconda3\\envs\\dsan6600_2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\xusha\\anaconda3\\envs\\dsan6600_2\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\xusha\\anaconda3\\envs\\dsan6600_2\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[102], line 37\u001b[0m, in \u001b[0;36mISIC_2024.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m,idx):\n\u001b[0;32m     36\u001b[0m     isic_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39miloc[idx,\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 37\u001b[0m     cleaned_image \u001b[38;5;241m=\u001b[39m \u001b[43mremove_hair\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpil_images\u001b[49m\u001b[43m[\u001b[49m\u001b[43misic_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(cleaned_image)\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "Cell \u001b[1;32mIn[102], line 11\u001b[0m, in \u001b[0;36mremove_hair\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m      8\u001b[0m blackhat \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mmorphologyEx(gray,cv2\u001b[38;5;241m.\u001b[39mMORPH_BLACKHAT,kernel)\n\u001b[0;32m     10\u001b[0m _, thresh \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mthreshold(blackhat, \u001b[38;5;241m10\u001b[39m ,\u001b[38;5;241m255\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mTHRESH_BINARY)\n\u001b[1;32m---> 11\u001b[0m inpainted_image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minpaint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthresh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mINPAINT_TELEA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inpainted_image\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for images, label, ids in train_loader:\n",
    "    print(ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsan6600_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
